# =============================================================================
# 1. 실행 및 데이터 관련 설정 (main.py, baseline.py 공용)
# =============================================================================
run:
  global_seed: 42 # 프로그램 전체의 재현성을 위한 전역 랜덤 시드. None으로 설정 시 비활성화.
  cuda: true # true: CUDA 사용 시도. false: CPU만 사용. CUDA를 사용할 수 없는 경우 자동으로 CPU로 전환됩니다.
  mode: 'inference' # 실행 모드. 'train': 모델 훈련, 'inference': 모델 추론 및 성능 평가
  
  pth_inference_dir: './pretrained/proposed_model'   # 추론 모드에서 사용할 .pth 모델이 포함된 디렉토리의 경로
  pth_best_name: 'best_model.pth' # .pth 모델의 파일 이름 또는 추론 시 불러올 모델의 파일 이름.
  evaluate_onnx: false # true: 훈련/추론 시 ONNX를 생성하고 성능을 평가합니다. (이미 각 pth 파일폴더에 생성 후 저장해뒀습니다.)

  # .onnx 파일 경로를 지정하면, inference 모드에서 PyTorch 모델 평가/변환을 건너뛰고 해당 ONNX 모델을 직접 평가합니다.
  # onnx_inference_path: 예: './log/Sewer-TAPNEW/run_20251201_095258/model_20251201_095320.onnx' # 평가할 ONNX 파일 경로
  
  # 정답 레이블이 없는 이미지에 대하여 추론만 수행하게 하는 옵션
  # 이를 true 로 활성화한다면, 아래의 train_split_ratio를 0.0으로 지정해야 합니다. (훈련데이터로 할당할 데이터 비율을 0.0으로 지정)
  only_inference: true # false: 정답 레이블과 비교하여 성능 평가.
  
  # 추론 시 양자화 옵션 (동시 사용 불가)
  use_fp16_inference: false # true: FP16(Half Precision) 추론 사용 (CUDA 권장)
  use_int8_inference: false # true: INT8 동적 양자화 추론 사용 (CPU 권장)

  # 로그를 콘솔에 출력하고 파일로 저장
  show_log: true # false: 모든 로깅 비활성화.
  
  # --- 데이터셋 설정 ---
  dataset:
    name: 'Sewer-TAP' # 생성할 폴더 이름 지정
    type: 'image_folder'
    train_split_ratio: 0.8 # 'image_folder' 타입일 때 사용할 훈련 데이터 비율 (나머지는 테스트/검증 데이터가 됨)

    # 데이터셋 경로 설정
    paths:
      # 1. 'csv' 타입 데이터셋 경로
      train_img_dir: '/home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/train'
      valid_img_dir: '/home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid'
      test_img_dir: '/home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid'
      train_csv: '/home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Train.csv'
      valid_csv: '/home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv'
      test_csv: '/home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv'

      # 2. 'image_folder' 타입 데이터셋 경로
      # /home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAP
      # /home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW
      # img_folder: '/home/pi/Desktop/tap_new/data'
      img_folder: '/home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW'
      # img_folder: '/home/user/workspace/CHOI/Sewer-TAPNEW/data' # 5090서버 임시 데이터
      # img_folder: '/home/cau/workspace/data/Sewer/Sewer-TAPNEW' # 동국대 블랙월 서버

  # 각 데이터셋에서 무작위로 샘플링할 데이터 비율. 1.0이면 전체 데이터 사용.
  random_sampling_ratio:
    train: 1.0
    valid: 1.0
    test: 1.0
  num_workers: 16 # 데이터 로딩 시 사용할 CPU 프로세스 수. 0이면 메인 프로세스만 사용. (시스템 권장값으로 수정)

# =============================================================================
# 2. 훈련 하이퍼파라미터 (main.py 전용)
# =============================================================================
training_run:
  epochs: 90                 
  batch_size: 16                
  pre_trained: false             # true: ImageNet 사전 훈련 가중치 사용, false: 무작위 초기화 가중치 사용
  optimizer: 'adamw'            # 사용할 옵티마이저. 옵션: 'adamw', 'sgd', 'nadam', 'radam', 'rmsprop'
  lr: 0.001                     # 옵티마이저의 학습률.
  weight_decay: 0.0001             # AdamW 옵티마이저의 가중치 감쇠(Weight Decay) 값.
  loss_function: 'CrossEntropyLoss' # 사용할 손실 함수. 옵션: 'CrossEntropyLoss', 'BCEWithLogitsLoss', 'FocalLoss'
  label_smoothing: 0.1            # 레이블 스무딩(Label Smoothing) 값. 0.0은 비활성화. (과적합 방지, 모델 일반화 성능 향상)
  
  # --- 학습률 Warmup 설정 ---
  warmup:
    enabled: false               # true: Warmup 사용, false: 사용 안 함
    epochs: 5                   # Warmup을 진행할 에포크 수
    start_lr: 0.00001           # Warmup 시작 시의 학습률 (선형증가)

# --- 'BCEWithLogitsLoss' 사용 시 양성 클래스 가중치 ---
  # bce_pos_weight: 'auto'        # 'auto'로 설정 시 자동 계산, 특정 값(예: 9.0)으로 수동 설정 가능. (num_labels=2여도 코드가 자동으로 [B, 1]로 변환하여 처리)
  
  # --- FocalLoss 하이퍼파라미터 --- (loss_function이 'FocalLoss'일 때 사용)
  # focal_loss_alpha: 0.5          # 클래스 불균형 조절. (0.5: 균등, >0.5: Defect 중요도 상승(Recall 향상), <0.5: Normal 중요도 상승(Precision 향상))
  # focal_loss_gamma: 2.0           # 학습 집중도 조절. (0: 일반 CE Loss, >0: 어려운 샘플에 집중(과적합 완화), 값이 클수록 집중도 상승)

  # 사용할 학습률 스케줄러를 설정합니다.
  scheduler: 'cosineannealinglr' # 옵션: 'multisteplr', 'cosineannealinglr', 'none'
  scheduler_params:
    # CosineAnnealingLR: T_max (주기, 보통 epochs), eta_min (최소 학습률)
    T_max: 90
    eta_min: 0.0001
  best_model_criterion: 'val_loss'  # 최고 모델 저장 기준. 옵션: 'val_loss', 'F1_average', 'F1_Normal', 'F1_Defect'.


# =============================================================================
# 3. 훈련 하이퍼파라미터 (baseline.py 전용)
# =============================================================================
training_baseline:
  epochs: 10                  
  batch_size: 16               
  pre_trained: false             # true: ImageNet 사전 훈련 가중치 사용, false: 무작위 초기화 가중치 사용
  optimizer: 'adamw'            # 사용할 옵티마이저. 옵션: 'adamw', 'sgd', 'nadam', 'radam', 'rmsprop'
  lr: 0.001                     # 옵티마이저의 학습률.
  weight_decay: 0.0001             # AdamW 옵티마이저의 가중치 감쇠(Weight Decay) 값.
  loss_function: 'CrossEntropyLoss' # 사용할 손실 함수. 옵션: 'CrossEntropyLoss', 'BCEWithLogitsLoss', 'FocalLoss'
  label_smoothing: 0.1            # 레이블 스무딩(Label Smoothing) 값. 0.0은 비활성화. (과적합 방지, 모델 일반화 성능 향상)
  # --- 학습률 Warmup 설정 ---
  warmup:
    enabled: false               # true: Warmup 사용, false: 사용 안 함
    epochs: 5                   # Warmup을 진행할 에포크 수
    start_lr: 0.00001           # Warmup 시작 시의 학습률 (선형증가)

  scheduler: 'cosineannealinglr' # 옵션: 'multisteplr', 'cosineannealinglr', 'none'
  scheduler_params:
    # CosineAnnealingLR: T_max (주기, 보통 epochs), eta_min (최소 학습률)
    T_max: 10
    eta_min: 0.0001
  best_model_criterion: 'val_loss'  # 최고 모델 저장 기준. 옵션: 'val_loss', 'F1_average', 'F1_Normal', 'F1_Defect'.

# =============================================================================
# 6. Pruning 후 미세 조정(Fine-tuning) 하이퍼파라미터 (baseline.py 전용)
# =============================================================================
finetuning_pruned:
  epochs: 80                     # Pruning 후 성능 복구를 위해 추가 훈련할 에포크 수.
  batch_size: 16                # 미세 조정 시 사용할 배치 크기.
  optimizer: 'adamw'             # 사용할 옵티마이저.
  lr: 0.001                     # 미세 조정 시에는 사전 훈련보다 낮은 학습률을 사용하는 것이 일반적입니다.
  weight_decay: 0.0001           # 가중치 감쇠 값.
  loss_function: 'CrossEntropyLoss' # 손실 함수.
  label_smoothing: 0.1           # 레이블 스무딩 값.
  # --- 학습률 Warmup 설정 ---
  warmup:
    enabled: false
    epochs: 5
    start_lr: 0.000001

  scheduler: 'cosineannealinglr'
  scheduler_params:
    T_max: 80 # fine-tuning epochs에 맞춰 조정
    eta_min: 0.0001
  best_model_criterion: 'val_loss'

# =============================================================================
# 4. 모델 아키텍처 파라미터 (main.py 전용)
# =============================================================================
model:
  # --- 이미지 및 인코더 설정 ---
  img_size: 224                # 모델에 입력하기 전 이미지의 리사이즈 크기 (정사각형).
  patch_size: 56              # 이미지를 나눌 정사각형 패치의 크기. `img_size`는 `patch_size`로 나누어 떨어져야 합니다.
  stride: 56                   # 패치를 추출할 때의 보폭(stride). `patch_size`보다 작으면 오버랩됩니다.
  cnn_feature_extractor:
    name: 'efficientnet_b0_feat2'    

  # --- 디코더 및 임베딩 설정 ---
  featured_patch_dim: 24     # CNN 특징 추출기가 각 이미지 패치에서 추출하는 특징 벡터의 차원.
  emb_dim: 24                # 트랜스포머 모델 내부에서 사용하는 통일된 임베딩 차원.
  num_heads: 2               # 멀티헤드 어텐션에서 사용할 헤드의 수. `emb_dim`은 `num_heads`로 나누어 떨어져야 합니다.
  num_decoder_layers: 2      # 모델 내부의 디코더 레이어 수.
  num_decoder_patches: 1     # 디코더에서 사용할 학습 가능한 쿼리(패치)의 수. 모델의 표현력을 조절합니다.
  adaptive_initial_query: true # true: 학습 가능한 쿼리와 인코더 출력을 cross-attention하여 동적 초기 쿼리 생성, false: 고정된 학습 가능 쿼리 사용.
  decoder_ff_ratio: 2        # 트랜스포머의 피드포워드 네트워크(FFN) 내부 차원을 결정하는 비율 (decoder_ff_dim = emb_dim * decoder_ff_ratio).
  dropout: 0.1               # 모델 내부에 적용될 드롭아웃 비율 (분류기, FFN, 어텐션 출력에 적용).
  positional_encoding: true  # 이미지 패치 시퀀스 (인코더 출력)에 위치 인코딩 추가 여부. False로 설정하면 위치 정보를 사용하지 않습니다.
  res_attention: true        # 디코더 레이어 간에 어텐션 스코어를 전달하는 잔차 어텐션 사용 여부.
  save_attention: true       # 추론 시 어텐션 맵을 저장할지 여부. True로 설정하면 시각화에 사용할 수 있습니다.
  num_plot_attention: 0     # 어텐션 맵을 시각화하여 저장할 최대 샘플 수.

# =============================================================================
# 5. Baseline 모델 설정 (baseline.py 전용)
# =============================================================================
baseline:
  # 사용할 baseline 모델의 이름을 지정합니다.
  # 사용 가능한 옵션: 'efficientnet_b0', 'mobilenet_v4_s', 'xie2019', 'deit_tiny', 'mobile_vit_xxs'
  model_name: 'mobile_vit_xxs'

  # --- 경량화 옵션 ---
#   use_l1_pruning: True       # L1 Norm Pruning 적용 여부
#   use_fpgm_pruning: True     # FPGM Pruning (torch-pruning 라이브러리 사용) 적용 여부
  # use_wanda_pruning: True     # Wanda (Pruning by Weights and Activations, ICLR 2024) # 중요도를 $|Weight| \times ||Input Norm||$으로 정의합니다. 즉, 가중치가 작더라도 입력 신호($X$)가 강하게 들어오는 뉴런은 중요하다고 판단합니다.
  num_wanda_calib_samples: 1353 # Wanda Pruning 시 Calibration에 사용할 샘플 수. Sewer-TAPNEW: 1353, Sewer-ML: 4096
  
  # pruning_sparsity: 0.4756640625     # 수동으로 설정할 가지치기 희소도. 아래 target_flops가 0보다 클 경우 이 값은 무시됩니다.
  pruning_flops_target: 0.1829   # 목표 FLOPs(GFLOPs). 0보다 큰 값으로 설정 시, 이 목표에 가장 가까운 pruning_sparsity를 자동으로 계산합니다.
  # pruning_params_target: 0.031371     # 목표 파라미터 수(단위: M). 0보다 큰 값으로 설정 시, 이 목표에 가장 가까운 pruning_sparsity를 자동으로 계산합니다.
